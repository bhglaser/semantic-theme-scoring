{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Intangible Investment from 10-K Filings\n",
    "\n",
    "This notebook walks through the full n-gram pipeline applied to SEC 10-K filings.\n",
    "\n",
    "**Research question:** What fraction of a firm's SG&A spending goes toward intangible investment (R&D, brand capital, organizational capital) versus routine operating expenses?\n",
    "\n",
    "**Approach:** Extract noun phrases from 10-K filings, cluster them into semantic communities, hand-label those communities as types of intangible investment, then score each filing against the labeled taxonomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Make sure you've installed dependencies:\n",
    "```bash\n",
    "pip install -r ../../requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')  # Move to ngram_pipeline root\n",
    "print('Working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Text Extraction\n",
    "\n",
    "This stage reads raw 10-K files, strips HTML tags, and extracts the Item 7 (MD&A) section using configurable regex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 01_extract_text.py --config examples/intangible_investment/config_intangible.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what was extracted\n",
    "import glob\n",
    "texts = glob.glob('output/intangible_investment/texts/*.txt')\n",
    "print(f'{len(texts)} text files extracted')\n",
    "if texts:\n",
    "    with open(texts[0]) as f:\n",
    "        content = f.read()\n",
    "    print(f'\\nFirst file: {os.path.basename(texts[0])}')\n",
    "    print(f'Length: {len(content.split())} words')\n",
    "    print(f'Preview: {content[:500]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: N-Gram Clustering\n",
    "\n",
    "Extracts POS-filtered bigrams/trigrams, embeds them, and clusters into semantic communities.\n",
    "\n",
    "**Note:** With a small sample corpus, you'll get fewer communities than the full analysis (which produced ~248 from 10,000 n-grams across thousands of filings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 02_cluster_ngrams.py --config examples/intangible_investment/config_intangible.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Look at top n-grams\n",
    "master = pd.read_csv('output/intangible_investment/clusters/ngram_master_list.csv')\n",
    "print(f'Top n-grams: {len(master)}')\n",
    "master.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at discovered communities\n",
    "labels = pd.read_csv('output/intangible_investment/clusters/community_results/community_labels_k500.csv')\n",
    "print(f'Communities discovered: {len(labels)}')\n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Labeling Step\n",
    "\n",
    "In the full analysis, we reviewed 231 communities and labeled each one. The `representatives` column shows the most central n-grams in each community.\n",
    "\n",
    "For example:\n",
    "- Community with representatives `advertising promotion expense, advertising expense cost` → **Intangible investment / brand or customer capital**\n",
    "- Community with representatives `function research development, research development work` → **Intangible investment / knowledge capital**\n",
    "- Community with representatives `cost office rent, rent expense office` → **Not intangible investment**\n",
    "\n",
    "The pre-labeled file is at `examples/intangible_investment/labeled_communities.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = pd.read_csv('examples/intangible_investment/labeled_communities.csv')\n",
    "print(f'Hand-labeled communities: {len(labeled)}')\n",
    "print(f'\\nCategory distribution:')\n",
    "print(labeled['category'].value_counts())\n",
    "print(f'\\nSubcategory distribution (within Intangible investment):')\n",
    "print(labeled[labeled['category'] == 'Intangible investment']['subcategory'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Per-Document N-Gram Extraction\n",
    "\n",
    "Re-extracts n-grams from each document and builds the master mapping from clustering output + labeled communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 03_extract_doc_ngrams.py --config examples/intangible_investment/config_intangible.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the master mapping\n",
    "mapping = pd.read_csv('output/intangible_investment/master_ngram_mapping.csv')\n",
    "print(f'Master mapping: {len(mapping)} n-grams mapped to categories')\n",
    "print(f'\\nCategory breakdown:')\n",
    "print(mapping['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Document Scoring\n",
    "\n",
    "Scores each document against the labeled communities using cosine-weighted n-gram counts, producing probability distributions over categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 04_score_documents.py --config examples/intangible_investment/config_intangible.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View final scores\n",
    "scores = pd.read_csv('output/intangible_investment/scores/scores_category_prob_embedding.csv')\n",
    "print(f'Documents scored: {len(scores)}')\n",
    "scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subcategory breakdown (within intangible investment)\n",
    "sub_scores = pd.read_csv('output/intangible_investment/scores/scores_subcategory_prob_embedding.csv')\n",
    "sub_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(scores) > 0:\n",
    "    score_cols = [c for c in scores.columns if c != 'doc_id']\n",
    "    means = scores[score_cols].mean()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    means.plot(kind='bar', ax=ax)\n",
    "    ax.set_ylabel('Average probability')\n",
    "    ax.set_title('Average category scores across documents')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting to Your Own Corpus\n",
    "\n",
    "To apply this pipeline to a different corpus or topic:\n",
    "\n",
    "1. **Replace the input documents** — put your text files in a directory and point `extract.input_dir` to it\n",
    "2. **Adjust the config** — disable section extraction if your documents don't have sections; update `custom_stop_words` for your domain\n",
    "3. **Run Stages 1-2** — this will discover communities specific to your corpus\n",
    "4. **Label the communities** — open the CSV, review representatives, add your own `category` and `subcategory` labels\n",
    "5. **Run Stages 3-4** — score your documents against your taxonomy\n",
    "\n",
    "The pipeline is domain-agnostic. The same algorithms work for news articles, scientific papers, legal documents, or any other text corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
